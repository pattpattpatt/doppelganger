{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fabe7460bd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pixel: tuple = (200, 100, 0)\n",
    "plt.imshow([[list(pixel)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutation_80_39.jpg\n",
      "534\n",
      "Mutation_10_32.jpg\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "# Get the minimum dimensions for cropping\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "min_height = 100000000\n",
    "min_width = 100000000\n",
    "min_width_filename = \"\"\n",
    "min_height_filename = \"\"\n",
    "\n",
    "mypath = \"/Users/alexp/dev/personal/doppelganger/training_sets/extended/set\"\n",
    "\n",
    "for f in listdir(mypath):\n",
    "    im = Image.open(join(mypath, f))\n",
    "    if im.size[0] < min_width:\n",
    "        min_width = im.size[0]\n",
    "        min_width_filename = f\n",
    "    if im.size[1] < min_height:\n",
    "        min_height = im.size[1]\n",
    "        min_height_filename = f\n",
    "\n",
    "print(min_height_filename)\n",
    "print(min_height)\n",
    "print(min_width_filename)\n",
    "print(min_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the images and store the modified images!\n",
    "# box = (0, 0, min_width, min_height)\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "mypath = \"/Users/alexp/dev/personal/doppelganger/training_sets/extended/set\"\n",
    "originals_path = \"/Users/alexp/dev/personal/doppelganger/training_sets/extended/data/originals\"\n",
    "mutations_path = \"/Users/alexp/dev/personal/doppelganger/training_sets/extended/data/mutations\"\n",
    "\n",
    "\n",
    "for f in listdir(mypath):\n",
    "    im = Image.open(join(mypath, f))\n",
    "    if re.search('Mutation', f) is None:\n",
    "        im.save(join(originals_path, f))\n",
    "    else:\n",
    "        im.save(join(mutations_path, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow import convert_to_tensor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "# Load modified images into memory!\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import random\n",
    "\n",
    "modified_path = \"/Users/alexp/dev/personal/doppelganger/training_sets/extended/modified\"\n",
    "image_data = { 'Test': [], 'Train': [] }\n",
    "count = 0\n",
    "for f in listdir(modified_path):\n",
    "    # Label Encoding:\n",
    "    # {\n",
    "    #   [0, 1]: Mutation,\n",
    "    #   [1, 0]: Original\n",
    "    # }\n",
    "    label = [1, 0] if re.search('%Mutation%', f) is None else [0, 1]\n",
    "    \n",
    "    # Bucket N% of images into training set vs test set\n",
    "    cohort = 'Train' if random.uniform(0, 1) < 0.7 else 'Test'\n",
    "\n",
    "    image_data[cohort].append({\n",
    "        'image': img_to_array(Image.open(join(modified_path, f))).astype(int),\n",
    "        'label': label\n",
    "    })\n",
    "print(len(image_data['Train']))\n",
    "print(len(image_data['Test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Training is done!\n",
      "Test data is done!\n",
      "436\n",
      "436\n",
      "167\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import convert_to_tensor\n",
    "from operator import itemgetter\n",
    "print(\"hello\")\n",
    "train_data = convert_to_tensor(list(map(itemgetter('image'), image_data['Train'])))\n",
    "train_labels = convert_to_tensor(list(map(itemgetter('label'), image_data['Train'])))\n",
    "print(\"Training is done!\")\n",
    "test_data = convert_to_tensor(list(map(itemgetter('image'), image_data['Test'])))\n",
    "test_labels = convert_to_tensor(list(map(itemgetter('label'), image_data['Test'])))\n",
    "print(\"Test data is done!\")\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(train_labels))\n",
    "print(len(test_data))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(534, 495, 3)\n",
      "tf.Tensor([1 0], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].shape)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modified images into memory!\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import random\n",
    "\n",
    "data_path = \"/Users/alexp/dev/personal/doppelganger/training_sets/extended/set\"\n",
    "count = 0\n",
    "for f in listdir(data_path):\n",
    "    label = [1, 0] if re.search('%Mutation%', f) is None else [0, 1]\n",
    "    \n",
    "    # Bucket N% of images into training set vs test set\n",
    "    cohort = 'Train' if random.uniform(0, 1) < 0.7 else 'Test'\n",
    "\n",
    "    image_data[cohort].append({\n",
    "        'image': img_to_array(Image.open(join(modified_path, f))).astype(int),\n",
    "        'label': label\n",
    "    })\n",
    "print(len(image_data['Train']))\n",
    "print(len(image_data['Test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 483 images belonging to 2 classes.\n",
      "Found 120 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Use keras data generators\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_dir = '../training_sets/extended/data'\n",
    "target_size = (534,495)\n",
    "classes = ('originals', 'mutations')\n",
    "class_mode = 'binary'\n",
    "interpolation = 'bicubic'\n",
    "\n",
    "generator = ImageDataGenerator(validation_split=0.2)\n",
    "train_generator = generator.flow_from_directory(\n",
    "    directory= data_dir,\n",
    "    target_size= target_size,\n",
    "    classes= classes,\n",
    "    class_mode= class_mode,\n",
    "    subset='training',\n",
    "    interpolation= interpolation,\n",
    "    save_to_dir='../training_sets/extended/augmented/train'\n",
    ")\n",
    "\n",
    "validation_generator = generator.flow_from_directory(\n",
    "    directory=data_dir,\n",
    "    target_size=target_size,\n",
    "    classes=classes,\n",
    "    class_mode=class_mode,\n",
    "    subset='validation',\n",
    "    interpolation= interpolation,\n",
    "    save_to_dir='../training_sets/extended/augmented/test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../training_sets/extended/augmented/test/_17_7408159.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9301c65af414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     63\u001b[0m         index_array = self.index_array[self.batch_size * idx:\n\u001b[1;32m     64\u001b[0m                                        self.batch_size * (idx + 1)]\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    248\u001b[0m                     \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                     format=self.save_format)\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;31m# build batch of labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2146\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../training_sets/extended/augmented/test/_17_7408159.png'"
     ]
    }
   ],
   "source": [
    "print(validation_generator[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 532, 493, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 266, 246, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 264, 244, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 132, 122, 64)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1030656)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                65962048  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 65,981,570\n",
      "Trainable params: 65,981,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model!\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(534,495,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "10/10 [==============================] - ETA: 0s - loss: 7.7015 - accuracy: 0.8750WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
      "10/10 [==============================] - 86s 9s/step - loss: 7.7015 - accuracy: 0.8750 - val_loss: 7.7022 - val_accuracy: 0.8833\n",
      "Epoch 2/15\n",
      "10/10 [==============================] - 55s 6s/step - loss: 7.7004 - accuracy: 0.8625\n",
      "Epoch 3/15\n",
      "10/10 [==============================] - 63s 6s/step - loss: 7.7012 - accuracy: 0.8719\n",
      "Epoch 4/15\n",
      "10/10 [==============================] - 55s 6s/step - loss: 7.7019 - accuracy: 0.8797\n",
      "Epoch 5/15\n",
      "10/10 [==============================] - 55s 6s/step - loss: 7.7022 - accuracy: 0.8832\n",
      "Epoch 6/15\n",
      "10/10 [==============================] - 64s 6s/step - loss: 7.7001 - accuracy: 0.8594\n",
      "Epoch 7/15\n",
      "10/10 [==============================] - 59s 6s/step - loss: 7.7007 - accuracy: 0.8656\n",
      "Epoch 8/15\n",
      "10/10 [==============================] - 64s 6s/step - loss: 7.7026 - accuracy: 0.8875\n",
      "Epoch 9/15\n",
      "10/10 [==============================] - 60s 6s/step - loss: 7.7019 - accuracy: 0.8797\n",
      "Epoch 10/15\n",
      "10/10 [==============================] - 61s 6s/step - loss: 7.7020 - accuracy: 0.8813\n",
      "Epoch 11/15\n",
      "10/10 [==============================] - 54s 5s/step - loss: 7.7034 - accuracy: 0.8969\n",
      "Epoch 12/15\n",
      "10/10 [==============================] - 59s 6s/step - loss: 7.7015 - accuracy: 0.8750\n",
      "Epoch 13/15\n",
      "10/10 [==============================] - 61s 6s/step - loss: 7.7010 - accuracy: 0.8694\n",
      "Epoch 14/15\n",
      "10/10 [==============================] - 59s 6s/step - loss: 7.7029 - accuracy: 0.8906\n",
      "Epoch 15/15\n",
      "10/10 [==============================] - 60s 6s/step - loss: 7.7022 - accuracy: 0.8832\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "          train_generator,\n",
    "          steps_per_epoch=10,\n",
    "          epochs=15,\n",
    "          validation_data=validation_generator,\n",
    "          validation_steps=100)\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 7.7022247314453125\n",
      "Test accuracy: 0.8833333253860474\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(validation_generator, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
